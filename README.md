# Tokenizer_Ablation

Human uses natural language and computer uses binary numbers. 
In order to calculate human language through a computer, natural language has to be converted into a series of numbers.

Tokenization is the first step in natural language processing.

An article on the concept of tokenization can be found in notion.
This repo concentrates on performing tokenization and comparing performance using various toolkits rather than clearing up the concepts.

This repo covers a series of Experiments on **Tokenizers**. To concentrate only on Tokenizers, all other experimental setups are fixed equally.

</br>

## Tokenizer desc

| &nbsp; **Language Pairs** &nbsp; | &nbsp; **Dataset** &nbsp; | **Tookit** | &nbsp; **Vocab** &nbsp; |
| :---: | :---: | :---: | :---: |
| En-De | Multi30k | &nbsp; SentencePiece &nbsp; | &nbsp; Integrated &nbsp; |
| - | - | - | Seperated |
| De-En | Multi30k | &nbsp; SentencePiece &nbsp; | Integrated |
| - | - | - | Seperated |
| Ko-En | AI Hub | &nbsp; SentencePiece / Mecab &nbsp; | Integrated |
| - | - | - | Seperated |
| En-Ko | AI Hub | &nbsp; SentencePiece / Mecab &nbsp; | Integrated |
| - | - | - | Seperated |

</br>
</br>

## Experimental Setups
**Model**
* Transformer

**Dataset**
* Multi30k
* AI Hub


</br>
</br>

## Results

</br>
</br>

## References
